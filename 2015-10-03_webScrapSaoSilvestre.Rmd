```{r header, include=FALSE}
##======================================================================
## Web scraping dos resultados da São Silvestre de 2013
## http://blog.leg.ufpr.br/archives/250
##                                            Walmes Zeviani - LEG UFPR
##======================================================================
```

```{r setupknitr, include=FALSE, purl=FALSE}
## ATTENTION!
opts_knit$set(
    base.dir="/home/walmes/Dropbox/blog/public/",
    base.url="http://blog.leg.ufpr.br/~walmes/public/")

opts_chunk$set(
    ## comment=NA,
    ## cache=TRUE,
    message=FALSE,
    error=FALSE,
    warning=FALSE,
    fig.width=6,
    fig.height=6,
    dpi=90,
    dev="png",
    dev.args=list(family="Helvetica"),
    fig.path="webScrapSaoSilvestre-")

```

Não é novidade que a internet é rica em informação. Contém textos e mais
textos sobre tudo. Mas não é só por meio dessa informação, em forma de
texto, que podemos aprender. A internet contém também muita informação
crua na forma de dados.

Ao contrário dos textos, a informação crua de dados, conjuntos de
números, requer coleta, processamento e análise para se transformar em
informação verbal. Quando os dados estão reunidos, e.g. dentro de um
único arquivo, não existe problema de coleta. Você baixa esse arquivo e
começa a processar e analisar de imediato. Por outro lado, muitos dados
estão disponíveis de uma maneira fragmentada e a coleta deles pode ser
um desafio.

A corrida de [São Silvestre] têm resultados disponíveis na internet
desde 1998. A partir de 2007, os resultados são centenas de páginas html
com tabelas de 10 linhas, uma linha por atleta. Com os dados
fragmentados desse jeito, qualquer iniciativa do tipo
recorta-cola-edita-repete, algo que muitos usuários de planilha
eletrônica fazem todos os dias, é abandonada por ser impraticável,
inexequível.

Vou partir para uma solução computacional, afinal, o computador tem que
trabalhar para mim, e não eu para ele. Vou trabalhar com técnicas de web
scraping para extrair esse resultados de todas as páginas.

Programar é algo que eu gosto, pois apredendo e também exercito o que já
sei. Na pior das hipóteses, posso passar horas programando essa solução,
mas certamente será menos tempo do que fazer a terefa manualmente, algo
repetitivo e que não acrescenta conhecimento algum. Na melhor das
hipóteses, posso usar minha rotina para outras páginas e compartilhar
com outras pessoas (com você que está lendo essa matéria).

As páginas de resultado têm uma estrutura fixa que é mantida em todas.
Os resultados para o masculino de [2013] consistem de cabeçalho e rodapé
com informações de situação, botões para mudar de página, e a tabela que
quero no meio. Essa têm 10 linhas e 8 colunas que incluem informações
como nome, idade e tempo para cumprir a prova.

Eu visitei várias páginas para certificar que a estrutura era a mesma.
Então fui para a página 4 (não me pergunte porque essa) para usar o
*inspecionar elemento* do Firefox e encontrei a estrutura na qual
(figura abaixo) a tabela é um elemento html `<table>`, que tem 8
atributos, como a largura `<witdh='98%'>` e a cor de fundo `<bgcolor>`.

<img src="http://blog.leg.ufpr.br/~walmes/printScreenSaoSilvestre.png" width='90%'>

Carreguei os pacotes necessários e li a página para decodificar com
`xmlTreeParse()`. No `summary` tem-se o número de ocorrências de cada
tipo de elemento html na página. O campo que me interessa é o `table`
pois é ele que contém os dados que quero. Mas não existe apenas um
`table` (como eu gostaria) e sendo assim, eu tive que ser específico ao
escrever a minha consulta para trazer a tabela alvo ignorando as demais.

```{r}
##----------------------------------------------------------------------
## Pacotes necessários.

library(XML)
library(lattice)
library(latticeExtra)

##----------------------------------------------------------------------
## Ler uma página.

## Url para a 4 página dos resultados do masculino em 2013.
## NOTE: paste0() é para juntar a linha quebrada evitando ultrapassar a
## margem, coisa que procuro sempre evitar.
url <- paste0("http://www.yescom.com.br",
              "/codigo_comum/classificacao/codigo",
              "/p_classificacao03_v1.asp",
              "?evento_yescom_id=1511&tipo=3",
              "&tipo_do_evento_id=4128",
              "&PaginaAtual=4",
              "&faixa=&sexo=M&campo=&pesquisa=")

## Abre a url no navegador.
## browseURL(url)

## Ler (direto da web).
pr <- readLines(con=url)
str(pr)

## Outras opções para ler a página.
## library(RCurl)
## system.time(pr <- getURL(url, useragent="curl"))
## system.time(pr <- getURL(url, useragent="wget"))

## Não peça para ver no console. Salve em arquivo.
## cat(con=pr, file="pg0004.html")                ## Salva em arquivo.
## download.file(url=url, destfile="pg0004.html") ## Ou baixa direto.

##----------------------------------------------------------------------
## Examina e decodifica.

h <- htmlTreeParse(file=pr, asText=TRUE,
                   useInternalNodes=TRUE, encoding="utf-8")

## Ocorrência de cada typo de elemento.
summary(h)

```

Minha primeira tentativa foi tentar a função `XML::readHTMLTable()` que
extraí as tabelas (`table`) de páginas html. De início eu achei que não
tinha dado muito certo porque ela leu todas as ocorrências de `table`
deixando em uma lista (`tb`). Para estudar o que eu havia conseguido,
pedi classe (`class`) e depois dimensão (`dim`). Nisso vi que tinha um
`data.frame` no elemento 10 com as dimensões certas, 10 por 8. Era a
tabela que eu queria.

```{r}
##----------------------------------------------------------------------
## Lê as tabelas (<table>) de uma página html.

tb <- readHTMLTable(h, header=TRUE,
                    stringsAsFactors=FALSE,
                    as.data.frame=TRUE)
length(tb)
## sapply(tb, class)
## sapply(tb, dim)

## A tabela que eu quero.
tb[[10]]

```

O código até aqui já resolveu 1 problema: ler a tabela de uma página. No
entanto, não vou omitir minhas outras tentativas. Eu procurei um jeito
de especificar apenas a tabela de resultados que eu queria e consegui
filtrando para as tabelas que tinham `width='98%'` e `bgcolor`. Essas
são informações que vieram do inspecionar elemento que fiz (ver
imagem). Note como fazer para especificar dois atributos.

```{r}
##----------------------------------------------------------------------
## Fazer uma consulta mais específica.

nc <- getNodeSet(doc=h,
                 path="//table[@width='98%'][@bgcolor]",
                 fun=readHTMLTable, stringsAsFactors=FALSE)
tb <- nc[[1]]
str(tb)

```

Temos duas soluções para o problema 1, mas ainda resta o problema 2: ler
todas as páginas. Resolvi correr um *loop* no intervalo de páginas. Para
deixar meu *script* mais interessante e praticar um pouco mais, o número
total de páginas eu fiz questão de extrair. Usei o inspecionar elemento
para chegar à especificação abaixo.

```{r}
##----------------------------------------------------------------------
## Qual o número total de páginas?

## <span class="FontVermelha">1745</span>
pges <- xpathApply(doc=h,
                   path="//span[@class='FontVermelha']",
                   fun=xmlValue)
last <- max(as.numeric(unlist(pges)))
last

```

Para ler as `r last` páginas, eu precisei preparar os
endereços. Felizmente, o endereço de cada página contém o seu número
(`PaginaAtual=`) e eu só precisei alterá-lo. Criei uma função
(`getTable`), que ao receber o link (url), executa os passos: decodifica
a página, extraí a tabela e escreve num arquivo texto, acrescentando
novas linhas (`append=TRUE`). Eu incluí uma opção de `verbose` para ter
algum *status* de evolução do processo, algo que foi útil enquanto eu
fazia a função. Também incluí um indicador de progresso modo texto no
laço `for` que faz a extração dos dados.

O sexo também é parte do endereço (`sexo=`) então resolvi ler os
dois. Algo inesperado aconteceu quando eu experimentei a leitura com
poucas páginas. O que difere na url para masculino e feminino, nos
resultados gerais, não é o `sexo=M` e `sexo=F` que eu esperava mas o
`tipo=3` e `tipo=4`. Independente do sexo, as urls têm
`sexo=M`. Esquisito, não é mesmo? Mas nos resultados por faixa etária o
`sexo=F` aparece.

```{r, eval=FALSE}
##----------------------------------------------------------------------
## Cria todos os endereços que serão lidos.

url0 <- paste0("http://www.yescom.com.br",
               "/codigo_comum/classificacao/codigo",
               "/p_classificacao03_v1.asp",
               "?evento_yescom_id=1511&tipo=%d",         ## sexo=tipo
               "&tipo_do_evento_id=4128&PaginaAtual=%d", ## página
               "&faixa=&sexo=M&campo=&pesquisa=")

getTable <- function(url, verbose=TRUE){
    t0 <- Sys.time()
    ## pr <- readLines(con=url)
    library(RCurl)
    pr <- getURL(url, useragent="wget")
    h <- htmlTreeParse(file=pr, asText=TRUE,
                       useInternalNodes=TRUE,
                       encoding="utf-8")
    nc <- getNodeSet(doc=h,
                     path="//table[@width='98%'][@bgcolor]",
                     fun=readHTMLTable,
                     stringsAsFactors=FALSE)
    tb <- nc[[1]]
    if (verbose){
        cat("Reading a page...\n")
        t1 <- Sys.time()
        tdiff <- t1-t0
        units(tdiff) <- "secs"
        cat("Elapsed: ", c(tdiff), " secs\n")
    }
    write.table(x=tb,
                file=filename,
                append=TRUE,
                quote=FALSE,
                sep=";",
                fileEncoding="utf-8",
                row.names=FALSE,
                col.names=FALSE)
    invisible(NULL)
}

filename <- "saoSilvestre2013.csv"
if (file.exists(filename)){
    file.remove(filename)
}

pb <- txtProgressBar(min=1, max=last, style=3)
for (i in 1:last){
    url <- sprintf(fmt=url0, 3, i)
    getTable(url, verbose=FALSE)
    setTxtProgressBar(pb, i)
}
last <- 526 ## Total de páginas do femínino.
pb <- txtProgressBar(min=1, max=last, style=3)
for (i in 1:last){
    url <- sprintf(fmt=url0, 4, i)
    getTable(url, verbose=FALSE)
    setTxtProgressBar(pb, i)
}

file.info(filename)

## Poderia usar um *apply também.
## invisible(lapply(urls, FUN=getTable, verbose=TRUE))

```

O que domina o tempo para concluir a tarefa é a velocidade de conexão
com a internet. O processo levou 15 minutos com conexão cabeada.

O objetivo dessa matéria foi mostrar como extrair esses dados de página
html. Agora que temos os dados, vamos só fazer um acabamento: ler os
dados para certificar a integridade/qualidade e conhecer, mesmo que por
cima, um pouco sobre eles.

```{r}
##----------------------------------------------------------------------
## Lê o arquivo texto gerado.

dtfr <- read.table("saoSilvestre2013.csv",
                   header=FALSE, sep=";",
                   stringsAsFactors=FALSE)
names(dtfr) <- c("class", "num", "atl", "idade",
                 "fxet", "eqp", "tmp", "tmpl")

dtfr$tmp <- as.POSIXct(dtfr$tmp, format="%H:%M:%S")-
    as.POSIXct("00:00:00", format="%H:%M:%S")
units(dtfr$tmp) <- "mins"
dtfr$tmp <- as.numeric(dtfr$tmp)

## Prepara a variável sexo a partir de faixa etária.
dtfr$sexo <- substr(dtfr$fxet, 1, 1)
dtfr$sexo <- factor(dtfr$sexo, levels=c("M", "F"))

## Reescreve os níveis sem presença do sexo.
dtfr$fxet <- paste0("[", substr(dtfr$fxet, 2, 3), ", ",
                    substr(dtfr$fxet, 4, 5), "]")

## Cria um fator baseado na idade.
dtfr$eq <- equal.count(dtfr$idade, number=9, overlap=0.1)

## Diagrama de dispersão do tempo de prova contra a idade.
xyplot(tmp~idade|sexo, data=dtfr, layout=c(1, NA),
       ylab="Tempo para concluir a prova (min)",
       xlab="Idade do atleta (anos)")

densityplot(~tmp|eq, data=dtfr, as.table=TRUE, groups=sexo,
            strip=strip.custom(strip.levels=TRUE, strip.names=FALSE),
            xlab="Tempo para concluir a prova (min)",
            ylab="Densidade")

ecdfplot(~tmp|eq, data=dtfr, as.table=TRUE,
         strip=strip.custom(strip.levels=TRUE, strip.names=FALSE),
         groups=sexo, auto.key=TRUE,
         xlab="Tempo para concluir a prova (min)",
         ylab="Distribuição de frequências relativas acumulada")

```

Você pode adaptar esse *script* ([webScrapSaoSilvestre.R]) para os
resultados de outros anos da São Silvestre. No entanto, se você visitar
os outros links, vai perceber que os resultados em html começaram em
2007 e que mantiveram o padrão estrutural. Antes de 2007, eles eram
divulgados em formato texto de comprimento fixo e cada ano tinha uma
estrutura diferente. Para ler os últimos, você pode usar a `read.fwf()`.

[São Silvestre]: http://www.saosilvestre.com.br/resultados/
[2013]: http://www.yescom.com.br/codigo_comum/classificacao/codigo/p_classificacao03_v1.asp?tipo_do_evento_id=4128&tipo=3&evento_yescom_id=1511
[webScrapSaoSilvestre.R]: http://blog.leg.ufpr.br/~walmes/public/2015-10-03_webScrapSaoSilvestre.R

<!-- --------------------------------------------------------------- -->

```{r publish, eval=FALSE, include=FALSE, purl=FALSE}
##-------------------------------------------
## Compilar e enviar para o blog.
## ATTENTION!

library(knitr)
library(RWordPress)

## Nome do arquivo.
post <- "2015-10-03_webScrapSaoSilvestre.Rmd"

## Titulo do post.
title <- "Web scraping da Corrida de São Silvestre"

## Categoria(s) e palavras chaves.
categ <- c("computacional")
keywd <- c("xml")

## Sua pasta local, conteúdo a subir para a public_html.
localpublic <- "/home/walmes/Dropbox/blog/public"

## Forneça senha de usuário do blog.
pass <- scan(n=1, what=character())

## Digitar porta e depois endereço do servidor (para rsync).
serv <- scan(n=2, what=character())

##-------------------------------------------
## Para ver como está ficando.

knit2html(post)

##-------------------------------------------
## Enviar figuras para a servidora.

## Extraí o código R da matéria.
cmd <- paste("purl -n", post, "&& mv",
             sub("md$", "", post), localpublic)
system(cmd)

## Executa rsync para enviar figuras para o servidor.
cmd <- paste("rsync -avzh --progress --delete",
             sprintf("-e \"ssh -p %s\"", serv[1]), localpublic,
             sprintf("walmes@%s:/home/walmes/public_html/", serv[2]))
system(cmd)

##-------------------------------------------
## Enviar para o blog do leg.

options(WordpressLogin=c(walmes=pass),
        WordpressURL="http://blog.leg.ufpr.br/xmlrpc.php")

source("http://git.leg.ufpr.br/leg/legTools/raw/master/R/knit2wpCrayon.R")

## Usar na primeira vez.
## knit2wpCrayon(post, title=title,
##               action="newPost",
##               categories=categ, mt_keywords=keywd,
##               write=FALSE, upload=TRUE,
##               publish=FALSE)

## Usar quando atualizar o post.
knit2wpCrayon(post, title=title,
              action="editPost", postid=250,
              categories=categ, mt_keywords=keywd,
              write=FALSE, upload=TRUE,
              publish=FALSE)

```
